meanWait = getMeanWait(1, quitAfter, gammaPerStep)
meanWaitDiscount =  meanWait$discount * gamma ^ iti
pReward = getPreward(1, quitAfter, gammaPerStep)
# Qquit =  pReward * tokenValue * meanRewardDiscount / (1 - meanWaitDiscount)
Qquit =  sum(pReward * tokenValue * meanRewardDiscount * meanWaitDiscount ^ (0 : 10))
Qquit_[quitTick] = Qquit
if(quitTick > 1) {
Qwait = vector(length = (quitTick - 1))
for(j in 1 :  (quitTick - 1)){
meanReward = getMeanReward(j, quitAfter, gammaPerStep)
meanWait = getMeanWait(j, quitAfter, gammaPerStep)
meanRewardDiscount = meanReward$discount
meanWaitDiscount = meanWait$discount
pReward = getPreward(j, quitAfter, gammaPerStep)
Qwait[j] = tokenValue * meanRewardDiscount * pReward + Qquit * meanWaitDiscount
}
Qwait_[[quitTick]] = Qwait
}
}
plot(Qquit_)
rewardRate$HP[[2]] / (1 - gamma )
Qquit_[[2]]
rewardRate$HP[[2]] / (1 - gamma )* gamma
plot(Qquit_ / rewardRate$HP)
plot(10 / rewardRate$HP)
rewardRate$HP[[1]]
rewardRate$HP[[2]]
rewardRate$HP[[201]]
Qquit_[[20]]
Qquit_[[201]]
rewardRate$HP[[201]] / (1 - gamma)
plot(Qquit_)
t = 20
n = length(trialTicks$HP)
statProbHP = unlist(lapply(1 : (n-1), function(i) {
sum(rewardDelayPDF$HP[(i+1): n]) / sum(rewardDelayPDF$HP[2:n] * (1 : (n-1)))
}))
thisStatProbHP = statProbHP[1 : (t / 0.1)] / sum(statProbHP[1 : (t / 0.1)] )
thisStatProbHP
plot(thisStatProbHP)
getMeanWait(1, 20, gammaPerStep)
9.95
1 / 3 / 20
gamma = gammaPerStep^2 # discount per 1s
nGap = length(trialTicks$HP) - 1
nTick =  length(trialTicks$HP)
tickIdx = gapIdx + 1
quitGap = quitAfter / 0.1
quitTick = quitGap  + 1
thisStatProbHP
gapIdx = 1
quitAfter = 20
gamma = gammaPerStep^2 # discount per 1s
nGap = length(trialTicks$HP) - 1
nTick =  length(trialTicks$HP)
tickIdx = gapIdx + 1
quitGap = quitAfter / 0.1
quitTick = quitGap  + 1
meanWaitDiscount =sum(gamma ^ pmin(seq(0, nGap - gapIdx) * 0.1, quitAfter - gapIdx * 0.1) * rewardDelayPDF$HP[tickIdx : nTick]) /
sum(rewardDelayPDF$HP[tickIdx : nTick])
meanWaitTime = sum(pmin(seq(0, nGap - gapIdx) * 0.1, quitAfter - gapIdx * 0.1) * rewardDelayPDF$HP[tickIdx : nTick]) /
sum(rewardDelayPDF$HP[tickIdx : nTick])
pmin(seq(0, nGap - gapIdx) * 0.1, quitAfter - gapIdx * 0.1
pmin(seq(0, nGap - gapIdx) * 0.1, quitAfter - gapIdx * 0.1)
rewardRate$HP[201]
10 / rewardRate$HP[201]
sum(pmin(seq(0, nGap - gapIdx) * 0.1, quitAfter - gapIdx * 0.1) * rewardDelayPDF$HP[tickIdx : nTick]) /
sum(rewardDelayPDF$HP[tickIdx : nTick])
getMeanWait = function(gapIdx, quitAfter, gammaPerStep){
gamma = gammaPerStep^2 # discount per 1s
nGap = length(trialTicks$HP) - 1
nTick =  length(trialTicks$HP)
tickIdx = gapIdx + 1
quitGap = quitAfter / 0.1
quitTick = quitGap  + 1
if(quitTick < tickIdx){
meanWaitDiscount = NaN
meanWaitTime = NaN
}else{
meanWaitDiscount =sum(gamma ^ pmin(seq(0, nGap - gapIdx) * 0.1 + 0.1, quitAfter - gapIdx * 0.1 + 0.1) * rewardDelayPDF$HP[tickIdx : nTick]) /
sum(rewardDelayPDF$HP[tickIdx : nTick])
meanWaitTime = sum(pmin(seq(0, nGap - gapIdx) * 0.1 + 0.1, quitAfter - gapIdx * 0.1 + 0.1) * rewardDelayPDF$HP[tickIdx : nTick]) /
sum(rewardDelayPDF$HP[tickIdx : nTick])
}
meanWait = list("discount" = meanWaitDiscount,
"time" = meanWaitTime)
return(meanWait)
}
# not so good to generate varibale in scripts, since sometimes it invloves variables you don't wont
######## condition varibles #########
conditions = c("HP", "LP")
conditionColors = c("#008837", "#7b3294")
######## timing variables ########
tMaxs = c(20, 40) # trial durations
blockMins = 7 # block duration in mins
blockSecs = blockMins * 60 # block duration in secs
iti = 2 # iti duration in secs
tGrid = seq(0, blockSecs, 0.1)
######### reward variable ########
tokenValue = 10 #value of the token
stepDuration = 0.5
########## supporting vairbales ########
# time ticks within a trial for timeEarnings or wtw analysis
trialTicks = list(
'HP' = round(seq(0, tMaxs[1], by = 0.1), 1),
'LP' = round(seq(0, tMaxs[2], by = 0.1), 1)
)
########## additional  variables for optimal analysis ########
# CDF of reward delays: p(t_reward <= T)
k = 4
mu = 0
sigma = 2
pareto = list()
pareto[['k']] = k
pareto[['mu']] = mu
pareto[['sigma']] = sigma
HP = 1 / trialTicks$HP[length(trialTicks$HP)]  * trialTicks$HP
LP = 1 - (1 + k * (trialTicks$LP - mu) / sigma) ^ (-1 / k)
LP[length(trialTicks$LP)] = 1
rewardDelayCDF = list(
HP = HP,
LP = LP
)
# library(ggplot2)
# source('plotThemes.R')
# plotData = data.frame(time = c(trialTicks$HP, trialTicks$LP),
#                       cdf = c(rewardDelayCDF$HP, rewardDelayCDF$LP),
#                       condition = c(rep('HP', length(trialTicks$HP)),
#                                     rep('LP', length(trialTicks$LP))))
# ggplot(plotData, aes(time, cdf, linetype = condition)) + geom_line() + xlab('Elapsed time / s') +
#   ylab('CDF') + ggtitle('Timing conditions') + saveTheme
#
# ggsave('../outputs/exp_figures/timing_conditions.png', width = 3, height = 2)
#  PDF of reward delays: p(t_reward = T)
#  make it discrete
HP = c(0, diff(rewardDelayCDF$HP))
LP = c(0, diff(rewardDelayCDF$LP))
rewardDelayPDF = list(
"HP" = HP,
"LP" = LP
)
# E(t_reward | t_reward <= T)
HP = cumsum(trialTicks$HP * rewardDelayPDF$HP) / cumsum(rewardDelayPDF$HP)
HP[1] = NaN
LP = cumsum(trialTicks$LP * rewardDelayPDF$LP) / cumsum(rewardDelayPDF$LP)
LP[1] = NaN
# no reward arrives before the first reward timing, so points before that turn to NAN
meanRewardDelay = list('HP' = HP, 'LP' = LP)
# rewardRate
HP = tokenValue * rewardDelayCDF$HP /
(meanRewardDelay$HP * rewardDelayCDF$HP + trialTicks$HP * (1 - rewardDelayCDF$HP) + iti)
LP = tokenValue * rewardDelayCDF$LP /
(meanRewardDelay$LP * rewardDelayCDF$LP + trialTicks$LP * (1 - rewardDelayCDF$LP) + iti)
# quitting before the first reward timing get 0 reward
HP[which(is.nan(HP))] = 0
LP[which(is.nan(LP))] = 0
rewardRate = list('HP' = HP, 'LP' = LP)
optimWaitTimes = list()
optimWaitTimes$HP = trialTicks$HP[which.max(HP)]
optimWaitTimes$LP = trialTicks$LP[which.max(LP)]
optimRewardRates = list()
optimRewardRates$HP = max(HP)
optimRewardRates$LP = max(LP)
# calculate wInis
wInisTheory = vector()
for(c in 1 : 2){
cond = conditions[c];
trialTick = trialTicks[[cond]]
thisDelayPDF = rewardDelayPDF[[cond]]
nTicks = length(trialTick)
# assume gamma = 0.9
gamma = 0.90
r = - log(gamma) / stepDuration
actionValueWaits = rep(0, nTicks)
for(k in 1 : nTicks){
actionValueWaits[k] = sum(tokenValue * exp(- (trialTick[k : nTicks] - trialTick[k]) * r)* thisDelayPDF[k : nTicks] / sum( thisDelayPDF[k : nTicks]))
}
junk = mean(actionValueWaits)
wInisTheory[c] = junk
}
wInis = vector(length = 2)
wInis[1] = 3 # value of waiting, since participants didn't know differences in conditions
wInis[2] = 2 # value of quitting, ensuring waiting first.
# calculate action value in the Q learning
getMeanReward= function(gapIdx, quitAfter, gammaPerStep){
gamma = gammaPerStep^2 # discount per 1s
nGap = length(trialTicks$HP) - 1
nTick =  length(trialTicks$HP)
tickIdx = gapIdx + 1
quitGap = quitAfter / 0.1
quitTick = quitGap  + 1
if(quitTick < tickIdx){
meanRewardDiscount = NaN
meanRewardTime = NaN
}else{
meanRewardDiscount =sum(gamma ^ (seq(0, quitGap - gapIdx) * 0.1 + 0.1) * rewardDelayPDF$HP[tickIdx : quitTick]) /
sum(rewardDelayPDF$HP[tickIdx : quitTick])
meanRewardTime = sum((seq(0, quitGap - gapIdx) * 0.1 + 0.1) * rewardDelayPDF$HP[tickIdx : quitTick]) /
sum(rewardDelayPDF$HP[tickIdx : quitTick])
}
meanReward = list("discount" = meanRewardDiscount,
"time" = meanRewardTime)
return(meanReward)
}
getMeanWait = function(gapIdx, quitAfter, gammaPerStep){
gamma = gammaPerStep^2 # discount per 1s
nGap = length(trialTicks$HP) - 1
nTick =  length(trialTicks$HP)
tickIdx = gapIdx + 1
quitGap = quitAfter / 0.1
quitTick = quitGap  + 1
if(quitTick < tickIdx){
meanWaitDiscount = NaN
meanWaitTime = NaN
}else{
meanWaitDiscount =sum(gamma ^ pmin(seq(0, nGap - gapIdx) * 0.1 + 0.1, quitAfter - gapIdx * 0.1 + 0.1) * rewardDelayPDF$HP[tickIdx : nTick]) /
sum(rewardDelayPDF$HP[tickIdx : nTick])
meanWaitTime = sum(pmin(seq(0, nGap - gapIdx) * 0.1 + 0.1, quitAfter - gapIdx * 0.1 + 0.1) * rewardDelayPDF$HP[tickIdx : nTick]) /
sum(rewardDelayPDF$HP[tickIdx : nTick])
}
meanWait = list("discount" = meanWaitDiscount,
"time" = meanWaitTime)
return(meanWait)
}
getPreward = function(gapIdx, quitAfter, gammaPerStep){
gamma = gammaPerStep^2 # discount per 1s
nGap = length(trialTicks$HP) - 1
nTick =  length(trialTicks$HP)
tickIdx = gapIdx + 1
quitGap = quitAfter / 0.1
quitTick = quitGap  + 1
pReward = (rewardDelayCDF$HP[quitTick] - rewardDelayCDF$HP[tickIdx - 1]) / (1 - rewardDelayCDF$HP[tickIdx - 1])
return(pReward)
}
# calculate Qwait for inf trials
gammaPerStep = 0.99 # discount per 0.5s
nGap = length(trialTicks$HP) - 1
nTick =  length(trialTicks$HP)
Qquit_ = vector(length = nGap)
Qwait_ = vector(mode = "list", length = nTick)
Qwait_[[1]] = NaN
# t means quitting after t
for(quitTick in 1 : nTick){
quitAfter = 0.1 * (quitTick-1)
meanReward = getMeanReward (1, quitAfter, gammaPerStep)
meanRewardDiscount =  meanReward$discount * gamma ^ iti
meanWait = getMeanWait(1, quitAfter, gammaPerStep)
meanWaitDiscount =  meanWait$discount * gamma ^ iti
pReward = getPreward(1, quitAfter, gammaPerStep)
# Qquit =  pReward * tokenValue * meanRewardDiscount / (1 - meanWaitDiscount)
Qquit =  sum(pReward * tokenValue * meanRewardDiscount * meanWaitDiscount ^ (0 : 10))
Qquit_[quitTick] = Qquit
if(quitTick > 1) {
Qwait = vector(length = (quitTick - 1))
for(j in 1 :  (quitTick - 1)){
meanReward = getMeanReward(j, quitAfter, gammaPerStep)
meanWait = getMeanWait(j, quitAfter, gammaPerStep)
meanRewardDiscount = meanReward$discount
meanWaitDiscount = meanWait$discount
pReward = getPreward(j, quitAfter, gammaPerStep)
Qwait[j] = tokenValue * meanRewardDiscount * pReward + Qquit * meanWaitDiscount
}
Qwait_[[quitTick]] = Qwait
}
}
plot(Qquit_)
plot(Qquit_)
plot(Qquit_ / rewardRate$HP)
quitTick = 20
quitTick = 201
quitAfter = 0.1 * (quitTick-1)
meanReward = getMeanReward (1, quitAfter, gammaPerStep)
meanRewardDiscount =  meanReward$discount * gamma ^ iti
meanWait = getMeanWait(1, quitAfter, gammaPerStep)
meanWaitDiscount =  meanWait$discount * gamma ^ iti
pReward = getPreward(1, quitAfter, gammaPerStep)
Qquit =  pReward * tokenValue * meanRewardDiscount / (1 - meanWaitDiscount)
# Qquit =  sum(pReward * tokenValue * meanRewardDiscount * meanWaitDiscount ^ (0 : 10))
Qquit
optimRewardRates$HP[201] / (1 - gamma)
optimRewardRates$HP[201]
optimRewardRates$HP
optimRewardRates$HP  / (1 - gamma)
pReward * tokenValue / meanWait$time
meanWait$time
pReward
pReward * tokenValue / (meanWait$time + 2)
pReward * tokenValue / (1 - meanWaitDiscount)
0.8298755 / (1 - gamma)gamma
0.8298755 / (1 - gamma)
pReward * tokenValue * meanRewardDiscount / (meanWait$time)
0.6629899 / (1 - gamma)
meanRewardDiscount
gamma
gammaPerStep = 0.99 # discount per 0.5s
nGap = length(trialTicks$HP) - 1
nTick =  length(trialTicks$HP)
Qquit_ = vector(length = nGap)
Qwait_ = vector(mode = "list", length = nTick)
Qwait_[[1]] = NaN
# t means quitting after t
quitAfter = 0.1 * (quitTick-1)
meanReward = getMeanReward (1, quitAfter, gammaPerStep)
meanRewardDiscount =  meanReward$discount * gamma ^ iti
meanWait = getMeanWait(1, quitAfter, gammaPerStep)
meanWaitDiscount =  meanWait$discount * gamma ^ iti
pReward = getPreward(1, quitAfter, gammaPerStep)
Qquit =  pReward * tokenValue * meanRewardDiscount / (1 - meanWaitDiscount)
# Qquit =  sum(pReward * tokenValue * meanRewardDiscount * meanWaitDiscount ^ (0 : 10))
meanRewardDiscount
meanReward$time
10 / (1 - meanRewardDiscount)
Qquit
optimRewardRates$HP
optimRewardRates$HP / (1 - gamma)
pReward * tokenValue * meanRewardDiscount
6.663048 + 6.663048 * meanWaitDiscount
rm(list = ls())
# not so good to generate varibale in scripts, since sometimes it invloves variables you don't wont
######## condition varibles #########
conditions = c("HP", "LP")
conditionColors = c("#008837", "#7b3294")
######## timing variables ########
tMaxs = c(20, 40) # trial durations
blockMins = 7 # block duration in mins
blockSecs = blockMins * 60 # block duration in secs
iti = 2 # iti duration in secs
tGrid = seq(0, blockSecs, 0.1)
######### reward variable ########
tokenValue = 10 #value of the token
stepDuration = 0.5
########## supporting vairbales ########
# time ticks within a trial for timeEarnings or wtw analysis
trialTicks = list(
'HP' = round(seq(0, tMaxs[1], by = 0.1), 1),
'LP' = round(seq(0, tMaxs[2], by = 0.1), 1)
)
########## additional  variables for optimal analysis ########
# CDF of reward delays: p(t_reward <= T)
k = 4
mu = 0
sigma = 2
pareto = list()
pareto[['k']] = k
pareto[['mu']] = mu
pareto[['sigma']] = sigma
HP = 1 / trialTicks$HP[length(trialTicks$HP)]  * trialTicks$HP
LP = 1 - (1 + k * (trialTicks$LP - mu) / sigma) ^ (-1 / k)
LP[length(trialTicks$LP)] = 1
rewardDelayCDF = list(
HP = HP,
LP = LP
)
# library(ggplot2)
# source('plotThemes.R')
# plotData = data.frame(time = c(trialTicks$HP, trialTicks$LP),
#                       cdf = c(rewardDelayCDF$HP, rewardDelayCDF$LP),
#                       condition = c(rep('HP', length(trialTicks$HP)),
#                                     rep('LP', length(trialTicks$LP))))
# ggplot(plotData, aes(time, cdf, linetype = condition)) + geom_line() + xlab('Elapsed time / s') +
#   ylab('CDF') + ggtitle('Timing conditions') + saveTheme
#
# ggsave('../outputs/exp_figures/timing_conditions.png', width = 3, height = 2)
#  PDF of reward delays: p(t_reward = T)
#  make it discrete
HP = c(0, diff(rewardDelayCDF$HP))
LP = c(0, diff(rewardDelayCDF$LP))
rewardDelayPDF = list(
"HP" = HP,
"LP" = LP
)
# E(t_reward | t_reward <= T)
HP = cumsum(trialTicks$HP * rewardDelayPDF$HP) / cumsum(rewardDelayPDF$HP)
HP[1] = NaN
LP = cumsum(trialTicks$LP * rewardDelayPDF$LP) / cumsum(rewardDelayPDF$LP)
LP[1] = NaN
# no reward arrives before the first reward timing, so points before that turn to NAN
meanRewardDelay = list('HP' = HP, 'LP' = LP)
# rewardRate
HP = tokenValue * rewardDelayCDF$HP /
(meanRewardDelay$HP * rewardDelayCDF$HP + trialTicks$HP * (1 - rewardDelayCDF$HP) + iti)
LP = tokenValue * rewardDelayCDF$LP /
(meanRewardDelay$LP * rewardDelayCDF$LP + trialTicks$LP * (1 - rewardDelayCDF$LP) + iti)
# quitting before the first reward timing get 0 reward
HP[which(is.nan(HP))] = 0
LP[which(is.nan(LP))] = 0
rewardRate = list('HP' = HP, 'LP' = LP)
optimWaitTimes = list()
optimWaitTimes$HP = trialTicks$HP[which.max(HP)]
optimWaitTimes$LP = trialTicks$LP[which.max(LP)]
optimRewardRates = list()
optimRewardRates$HP = max(HP)
optimRewardRates$LP = max(LP)
# calculate wInis
wInisTheory = vector()
for(c in 1 : 2){
cond = conditions[c];
trialTick = trialTicks[[cond]]
thisDelayPDF = rewardDelayPDF[[cond]]
nTicks = length(trialTick)
# assume gamma = 0.9
gamma = 0.90
r = - log(gamma) / stepDuration
actionValueWaits = rep(0, nTicks)
for(k in 1 : nTicks){
actionValueWaits[k] = sum(tokenValue * exp(- (trialTick[k : nTicks] - trialTick[k]) * r)* thisDelayPDF[k : nTicks] / sum( thisDelayPDF[k : nTicks]))
}
junk = mean(actionValueWaits)
wInisTheory[c] = junk
}
wInis = vector(length = 2)
wInis[1] = 3 # value of waiting, since participants didn't know differences in conditions
wInis[2] = 2 # value of quitting, ensuring waiting first.
# calculate action value in the Q learning
getMeanReward= function(gapIdx, quitAfter, gammaPerStep){
gamma = gammaPerStep^2 # discount per 1s
nGap = length(trialTicks$HP) - 1
nTick =  length(trialTicks$HP)
tickIdx = gapIdx + 1
quitGap = quitAfter / 0.1
quitTick = quitGap  + 1
if(quitTick < tickIdx){
meanRewardDiscount = NaN
meanRewardTime = NaN
}else{
meanRewardDiscount =sum(gamma ^ (seq(0, quitGap - gapIdx) * 0.1 + 0.1) * rewardDelayPDF$HP[tickIdx : quitTick]) /
sum(rewardDelayPDF$HP[tickIdx : quitTick])
meanRewardTime = sum((seq(0, quitGap - gapIdx) * 0.1 + 0.1) * rewardDelayPDF$HP[tickIdx : quitTick]) /
sum(rewardDelayPDF$HP[tickIdx : quitTick])
}
meanReward = list("discount" = meanRewardDiscount,
"time" = meanRewardTime)
return(meanReward)
}
getMeanWait = function(gapIdx, quitAfter, gammaPerStep){
gamma = gammaPerStep^2 # discount per 1s
nGap = length(trialTicks$HP) - 1
nTick =  length(trialTicks$HP)
tickIdx = gapIdx + 1
quitGap = quitAfter / 0.1
quitTick = quitGap  + 1
if(quitTick < tickIdx){
meanWaitDiscount = NaN
meanWaitTime = NaN
}else{
meanWaitDiscount =sum(gamma ^ pmin(seq(0, nGap - gapIdx) * 0.1 + 0.1, quitAfter - gapIdx * 0.1 + 0.1) * rewardDelayPDF$HP[tickIdx : nTick]) /
sum(rewardDelayPDF$HP[tickIdx : nTick])
meanWaitTime = sum(pmin(seq(0, nGap - gapIdx) * 0.1 + 0.1, quitAfter - gapIdx * 0.1 + 0.1) * rewardDelayPDF$HP[tickIdx : nTick]) /
sum(rewardDelayPDF$HP[tickIdx : nTick])
}
meanWait = list("discount" = meanWaitDiscount,
"time" = meanWaitTime)
return(meanWait)
}
getPreward = function(gapIdx, quitAfter, gammaPerStep){
gamma = gammaPerStep^2 # discount per 1s
nGap = length(trialTicks$HP) - 1
nTick =  length(trialTicks$HP)
tickIdx = gapIdx + 1
quitGap = quitAfter / 0.1
quitTick = quitGap  + 1
pReward = (rewardDelayCDF$HP[quitTick] - rewardDelayCDF$HP[tickIdx - 1]) / (1 - rewardDelayCDF$HP[tickIdx - 1])
return(pReward)
}
# calculate Qwait for inf trials
gammaPerStep = 0.99 # discount per 0.5s
nGap = length(trialTicks$HP) - 1
nTick =  length(trialTicks$HP)
Qquit_ = vector(length = nGap)
Qwait_ = vector(mode = "list", length = nTick)
Qwait_[[1]] = NaN
# t means quitting after t
for(quitTick in 1 : nTick){
quitAfter = 0.1 * (quitTick-1)
meanReward = getMeanReward (1, quitAfter, gammaPerStep)
meanRewardDiscount =  meanReward$discount * gamma ^ iti
meanWait = getMeanWait(1, quitAfter, gammaPerStep)
meanWaitDiscount =  meanWait$discount * gamma ^ iti
pReward = getPreward(1, quitAfter, gammaPerStep)
Qquit =  pReward * tokenValue * meanRewardDiscount / (1 - meanWaitDiscount)
# Qquit =  sum(pReward * tokenValue * meanRewardDiscount * meanWaitDiscount ^ (0 : 10))
Qquit_[quitTick] = Qquit
if(quitTick > 1) {
Qwait = vector(length = (quitTick - 1))
for(j in 1 :  (quitTick - 1)){
meanReward = getMeanReward(j, quitAfter, gammaPerStep)
meanWait = getMeanWait(j, quitAfter, gammaPerStep)
meanRewardDiscount = meanReward$discount
meanWaitDiscount = meanWait$discount
pReward = getPreward(j, quitAfter, gammaPerStep)
Qwait[j] = tokenValue * meanRewardDiscount * pReward + Qquit * meanWaitDiscount
}
Qwait_[[quitTick]] = Qwait
}
}
plot(Qquit_)
meanRewardDelay$H
rewardDelayCDF
a = rep(0.005, 200)
b = seq(0.05, 19.95, by = 0.1)
length(b)
sum(a * b)
rewardDelayCDF$HP
trialDiscretes = list(
"HP" = round(seq(0.05, tMaxs[1] - 0.05, by = 0.1), 2),
"LP" = round(seq(0.05, tMaxs[2] - 0.05, by = 0.1), 2)
)
/ length(trialDiscretes$HP) * trialDiscretes$HP
1 / length(trialDiscretes$HP) * trialDiscretes$HP
length(trialDiscretes$HP)
1 / length(trialDiscretes$HP)
rm(list = ls())
