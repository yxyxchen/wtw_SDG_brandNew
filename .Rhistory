source("subFxs/plotThemes.R")
dir.create("figures/simDataAnalysis")
dirName = sprintf("figures/simDataAnalysis/%s", modelName)
dir.create(dirName)
for(c in 1 : 2){
cond = conditions[c]
if(cond == "HP") trialData = trialHPData else trialData = trialLPData
tMax = ifelse(cond == "HP", tMaxs[1], tMaxs[2])
# calculate AUC and timeWaited
plotKMSC = F
label = ""
kmGrid = seq(0, tMax, by=0.1)
# initialize
totalEarnings_ = matrix(0, nComb, nRep)
AUC_ = matrix(0, nComb, nRep)
for(sIdx in 1 : nComb){
for(rIdx in 1 : nRep){
thisTrialData = trialData[[simNo[sIdx, rIdx]]]
kmscResults = kmsc(thisTrialData,tMax,label,plotKMSC,kmGrid)
AUC_[sIdx, rIdx] = kmscResults[['auc']]
}
}
AUC = apply(AUC_, MARGIN = 1, FUN = mean)
if(cond == "HP") AUCHP = AUC else AUCLP = AUC
}
paras = getParas("R_learning2")
nPara = length(paras)
for(c in 1 : 2){
cond = conditions[c]
condColor = conditionColors[c]
ylimit = ifelse(cond == "HP", 25, 45)
if(cond == "HP") AUC = AUCHP else AUC = AUCLP
tempt = data.frame(paraComb, AUC = AUC)
for(i in 1 : nPara){
para = paras[i]
tempt1 = tempt %>% group_by_at(vars(para)) %>% summarise(mu = mean(AUC))
tempt2 = tempt %>% group_by_at(vars(para)) %>% summarise(std = sd(AUC))
plotData = data.frame(tempt1, std = tempt2$std)
plotData[[para]]= as.factor(plotData[[para]])
plotData$ymin = plotData$mu - plotData$std
plotData$ymax = plotData$mu + plotData$std
ggplot(plotData, aes_string(para, "mu")) +
geom_bar(stat = "identity", color = condColor, fill = condColor) +
saveTheme + xlab(capitalize(para)) + ylab("AUC / min") + ylim(c(-3, ylimit)) +
geom_errorbar(aes(ymin = ymin, ymax = ymax), width = 0.2)
fileName = sprintf("figures/simDataAnalysis/%s/AUC_%s_%s.pdf", modelName, cond, para)
ggsave(fileName, width = 3, height = 4)
}
}
trialData = trialLPData
paraTable = data.frame(phi1 = c(0.02, 0.05, 0.08), phi2 = c(0.02, 0.05, 0.08),tau = c(5, 10, 15))
paraComb = getParaComb(paraTable)
nTimeStep = 80
for(i in 1 : nComb){
thisTrialData = trialData[[i]]
# actionValueViewer(thisTrialData$vaWaits, thisTrialData$vaRewardRates, thisTrialData)
# plotData = data.frame(time = (1 : nTimeStep) * stepDuration, Qwait = thisTrialData$vaWaits[,50])
# p=ggplot(plotData, aes(time, Qwait)) + ggtitle(paste(paraComb[i,], collapse = " ")) + geom_point()
# print(p)
print(mean(thisTrialData$trialEarnings / (thisTrialData$timeWaited * 2  + 4 )))
readline("continue")
}
# look at actions
trialData = trialLPData
paraTable = data.frame(phi1 = c(0.02, 0.05, 0.08), phi2 = c(0.02, 0.05, 0.08),tau = c(5, 10, 15))
paraComb = getParaComb(paraTable)
nTimeStep = 80
for(i in 1 : nComb){
thisTrialData = trialData[[i]]
actionValueViewer(thisTrialData$vaWaits, thisTrialData$vaRewardRates, thisTrialData)
# plotData = data.frame(time = (1 : nTimeStep) * stepDuration, Qwait = thisTrialData$vaWaits[,50])
# p=ggplot(plotData, aes(time, Qwait)) + ggtitle(paste(paraComb[i,], collapse = " ")) + geom_point()
# print(p)
#print(mean(thisTrialData$trialEarnings / (thisTrialData$timeWaited * 2  + 4 )))
readline("continue")
}
4.4 / 4
4.5 / 4
source('~/Documents/first_kick/wtw_SDG_brandNew/subFxs/repetitionFxs.R', echo=TRUE)
modelName = "R_learning2"
nBlock = 1
nRep = 10
paraTable = data.frame(phi1 = c(0.02, 0.05, 0.08), phi2 = c(0.02, 0.05, 0.08),tau = c(5, 10, 15))
simulate(modelName, nBlock, nRep, paraTable)
#
library("ggplot2")
library("Hmisc")
source("subFxs/analysisFxs.R")
source("subFxs/helpFxs.R")
source("subFxs/loadFxs.R")
load("wtwSettings.RData")
modelName = "R_learning2"
dir.create("figures/simDataAnalysis")
dirName = sprintf("genData/simulation/%s", modelName)
load(sprintf("%s/trialHPData.RData", dirName))
load(sprintf("%s/trialLPData.RData", dirName))
load(sprintf("%s/simParas.RData", dirName))
source("subFxs/plotThemes.R")
dir.create("figures/simDataAnalysis")
dirName = sprintf("figures/simDataAnalysis/%s", modelName)
dir.create(dirName)
for(c in 1 : 2){
cond = conditions[c]
if(cond == "HP") trialData = trialHPData else trialData = trialLPData
tMax = ifelse(cond == "HP", tMaxs[1], tMaxs[2])
# calculate AUC and timeWaited
plotKMSC = F
label = ""
kmGrid = seq(0, tMax, by=0.1)
# initialize
totalEarnings_ = matrix(0, nComb, nRep)
AUC_ = matrix(0, nComb, nRep)
for(sIdx in 1 : nComb){
for(rIdx in 1 : nRep){
thisTrialData = trialData[[simNo[sIdx, rIdx]]]
kmscResults = kmsc(thisTrialData,tMax,label,plotKMSC,kmGrid)
AUC_[sIdx, rIdx] = kmscResults[['auc']]
}
}
AUC = apply(AUC_, MARGIN = 1, FUN = mean)
if(cond == "HP") AUCHP = AUC else AUCLP = AUC
}
paras = getParas("R_learning2")
nPara = length(paras)
for(c in 1 : 2){
cond = conditions[c]
condColor = conditionColors[c]
ylimit = ifelse(cond == "HP", 25, 45)
if(cond == "HP") AUC = AUCHP else AUC = AUCLP
tempt = data.frame(paraComb, AUC = AUC)
for(i in 1 : nPara){
para = paras[i]
tempt1 = tempt %>% group_by_at(vars(para)) %>% summarise(mu = mean(AUC))
tempt2 = tempt %>% group_by_at(vars(para)) %>% summarise(std = sd(AUC))
plotData = data.frame(tempt1, std = tempt2$std)
plotData[[para]]= as.factor(plotData[[para]])
plotData$ymin = plotData$mu - plotData$std
plotData$ymax = plotData$mu + plotData$std
ggplot(plotData, aes_string(para, "mu")) +
geom_bar(stat = "identity", color = condColor, fill = condColor) +
saveTheme + xlab(capitalize(para)) + ylab("AUC / min") + ylim(c(-3, ylimit)) +
geom_errorbar(aes(ymin = ymin, ymax = ymax), width = 0.2)
fileName = sprintf("figures/simDataAnalysis/%s/AUC_%s_%s.pdf", modelName, cond, para)
ggsave(fileName, width = 3, height = 4)
}
}
# look at actions
trialData = trialLPData
paraTable = data.frame(phi1 = c(0.02, 0.05, 0.08), phi2 = c(0.02, 0.05, 0.08),tau = c(5, 10, 15))
paraComb = getParaComb(paraTable)
nTimeStep = 80
for(i in 1 : nComb){
thisTrialData = trialData[[i]]
#actionValueViewer(thisTrialData$vaWaits, thisTrialData$vaRewardRates, thisTrialData)
plotData = data.frame(time = (1 : nTimeStep) * stepDuration, Qwait = thisTrialData$vaWaits[,50])
p=ggplot(plotData, aes(time, Qwait)) + ggtitle(paste(paraComb[i,], collapse = " ")) + geom_point()
print(p)
#print(mean(thisTrialData$trialEarnings / (thisTrialData$timeWaited * 2  + 4 )))
readline("continue")
}
# look at actions
trialData = trialHPData
paraTable = data.frame(phi1 = c(0.02, 0.05, 0.08), phi2 = c(0.02, 0.05, 0.08),tau = c(5, 10, 15))
paraComb = getParaComb(paraTable)
nTimeStep = 40
for(i in 1 : nComb){
thisTrialData = trialData[[i]]
#actionValueViewer(thisTrialData$vaWaits, thisTrialData$vaRewardRates, thisTrialData)
plotData = data.frame(time = (1 : nTimeStep) * stepDuration, Qwait = thisTrialData$vaWaits[,50])
p=ggplot(plotData, aes(time, Qwait)) + ggtitle(paste(paraComb[i,], collapse = " ")) + geom_point()
print(p)
#print(mean(thisTrialData$trialEarnings / (thisTrialData$timeWaited * 2  + 4 )))
readline("continue")
}
optimRewardRates
10 / optimWaitRate
10 / optimRewardRates$LP
optimWaitTimes
# look at actions
trialData = trialHPData
paraTable = data.frame(phi1 = c(0.02, 0.05, 0.08), phi2 = c(0.02, 0.05, 0.08),tau = c(5, 10, 15))
paraComb = getParaComb(paraTable)
nTimeStep = 40
for(i in 1 : nComb){
thisTrialData = trialData[[i]]
actionValueViewer(thisTrialData$vaWaits, thisTrialData$vaRewardRates, thisTrialData)
# plotData = data.frame(time = (1 : nTimeStep) * stepDuration, Qwait = thisTrialData$vaWaits[,50])
# p=ggplot(plotData, aes(time, Qwait)) + ggtitle(paste(paraComb[i,], collapse = " ")) + geom_point()
# print(p)
#print(mean(thisTrialData$trialEarnings / (thisTrialData$timeWaited * 2  + 4 )))
readline("continue")
}
<<<<<<< HEAD
expPara = loadExpPara(modelName, paras)
useID = getUseID(blockData, expPara, paras)
plotParaAUC(expPara, "phi", blockData, useID)
plotParaAUC(expPara, "tau", blockData, useID)
plotParaAUC(expPara, "gamma", blockData, useID)
# library
library('ggplot2')
library('dplyr')
library('tidyr')
load("wtwSettings.RData")
source('subFxs/simulationFxs.R') #
source("subFxs/taskFxs.R")
source("subFxs/helpFxs.R")
source("subFxs/plotThemes.R")
cond = "HP"
para = c(0.1, 10, 0.9, 4)
nBlock = 1
set.seed(1)
source('~/Documents/first_kick/wtw_SDG_brandNew/subFxs/simulationFxs.R', echo=TRUE)
tempt = full_model(para, cond, nBlock)
tempt$scheduledWait
=======
source('~/Documents/first_kick/wtw_SDG_brandNew/subFxs/repetitionFxs.R', echo=TRUE)
source('~/Documents/first_kick/wtw_SDG_brandNew/subFxs/repetitionFxs.R', echo=TRUE)
modelName = "R_learning2"
nBlock = 1
nRep = 10
paraTable = data.frame(phi1 = c(0.02, 0.05, 0.08), phi2 = c(0.02, 0.05, 0.08),tau = c(5, 10, 15))
simulate(modelName, nBlock, nRep, paraTable)
>>>>>>> 984cd6e9baf96f49b559b8efde30be1d78b714d1
# not so good to generate varibale in scripts, since sometimes it invloves variables you don't wont
######## condition varibles #########
conditions = c("HP", "LP")
conditionColors = c("#008837", "#7b3294")
######## timing variables ########
tMaxs = c(20, 40) # trial durations
blockMins = 7 # block duration in mins
blockSecs = blockMins * 60 # block duration in secs
iti = 2 # iti duration in secs
tGrid = seq(0, blockSecs, 0.1)
######### reward variable ########
tokenValue = 10 #value of the token
stepDuration = 0.5
########## supporting vairbales ########
# time ticks within a trial for timeEarnings or wtw analysis
trialTicks = list(
'HP' = round(seq(0, tMaxs[1], by = 0.1), 1),
'LP' = round(seq(0, tMaxs[2], by = 0.1), 1)
)
########## additional  variables for optimal analysis ########
# CDF of reward delays: p(t_reward <= T)
k = 4
mu = 0
sigma = 2
pareto = list()
pareto[['k']] = k
pareto[['mu']] = mu
pareto[['sigma']] = sigma
HP = 1 / trialTicks$HP[length(trialTicks$HP)]  * trialTicks$HP
LP = 1 - (1 + k * (trialTicks$LP - mu) / sigma) ^ (-1 / k)
LP[length(trialTicks$LP)] = 1
rewardDelayCDF = list(
HP = HP,
LP = LP
)
# library(ggplot2)
# source('plotThemes.R')
# plotData = data.frame(time = c(trialTicks$HP, trialTicks$LP),
#                       cdf = c(rewardDelayCDF$HP, rewardDelayCDF$LP),
#                       condition = c(rep('HP', length(trialTicks$HP)),
#                                     rep('LP', length(trialTicks$LP))))
# ggplot(plotData, aes(time, cdf, linetype = condition)) + geom_line() + xlab('Elapsed time / s') +
#   ylab('CDF') + ggtitle('Timing conditions') + saveTheme
#
# ggsave('../outputs/exp_figures/timing_conditions.png', width = 3, height = 2)
#  PDF of reward delays: p(t_reward = T)
#  make it discrete
HP = c(0, diff(rewardDelayCDF$HP))
LP = c(0, diff(rewardDelayCDF$LP))
rewardDelayPDF = list(
"HP" = HP,
"LP" = LP
)
# E(t_reward | t_reward <= T)
HP = cumsum(trialTicks$HP * rewardDelayPDF$HP) / cumsum(rewardDelayPDF$HP)
HP[1] = NaN
LP = cumsum(trialTicks$LP * rewardDelayPDF$LP) / cumsum(rewardDelayPDF$LP)
LP[1] = NaN
# no reward arrives before the first reward timing, so points before that turn to NAN
meanRewardDelay = list('HP' = HP, 'LP' = LP)
# rewardRate
HP = tokenValue * rewardDelayCDF$HP /
(meanRewardDelay$HP * rewardDelayCDF$HP + trialTicks$HP * (1 - rewardDelayCDF$HP) + iti)
LP = tokenValue * rewardDelayCDF$LP /
(meanRewardDelay$LP * rewardDelayCDF$LP + trialTicks$LP * (1 - rewardDelayCDF$LP) + iti)
# quitting before the first reward timing get 0 reward
HP[which(is.nan(HP))] = 0
LP[which(is.nan(LP))] = 0
rewardRate = list('HP' = HP, 'LP' = LP)
optimWaitTimes = list()
optimWaitTimes$HP = trialTicks$HP[which.max(HP)]
optimWaitTimes$LP = trialTicks$LP[which.max(LP)]
optimRewardRates = list()
optimRewardRates$HP = max(HP)
optimRewardRates$LP = max(LP)
# calculate wInis
wInisTheory = vector()
for(c in 1 : 2){
cond = conditions[c];
trialTick = trialTicks[[cond]]
thisDelayPDF = rewardDelayPDF[[cond]]
nTicks = length(trialTick)
# assume gamma = 0.9
gamma = 0.90
r = - log(gamma) / stepDuration
actionValueWaits = rep(0, nTicks)
for(k in 1 : nTicks){
actionValueWaits[k] = sum(tokenValue * exp(- (trialTick[k : nTicks] - trialTick[k]) * r)* thisDelayPDF[k : nTicks] / sum( thisDelayPDF[k : nTicks]))
}
junk = mean(actionValueWaits)
wInisTheory[c] = junk
}
wInis = vector(length = 2)
wInis[1] = 3 # value of waiting, since participants didn't know differences in conditions
wInis[2] = 2 # value of quitting, ensuring waiting first.
# calculate action value in the R learning, given the policy is always wait
# remember, here the average reward is for per 0.1s, namely per tick, not per s
n = length(trialTicks$HP)
HP = unlist(lapply(1 : (n - 1), function(i){
values = tokenValue - (trialTicks$HP[i : (n - 1)] - 0.1 * i + 0.1 + iti) *  rewardRate$HP[n]
weights = rewardDelayPDF$HP[(i+1) : n]
sum(values * weights)  / sum(weights)
})) / 0.1
n = length(trialTicks$LP)
LP = unlist(lapply(1 : (n - 1), function(i){
values = tokenValue - (trialTicks$LP[i : (n - 1)] - 0.1 * i + 0.1 + iti) *  rewardRate$LP[n]
weights = rewardDelayPDF$LP[(i+1) : n]
sum(values * weights)  / sum(weights)
})) / 0.1
tList = seq(5, 20, by = 5)
nT = length(tList)
HP_ = vector(mode = "list", length = nT)
HPQquit_ = vector(mode = "list", length = nT)
for(h in 1: nT){
t =  tList[h]
n = length(trialTicks$HP)
HP = unlist(lapply(1 : (t / 0.1), function(i){
values = rep(-rewardRate$HP[t / 0.1 + 1] *  (t -  0.1 * i + iti), n - i)# when get no rewards, wait for vein for (t - 0.1 * i) gap duration
values[1 : (t / 0.1 + 1 - i)] = tokenValue - (0.1 * (0 : (t / 0.1 - i)) + iti)* rewardRate$HP[t / 0.1 + 1]
weights = rewardDelayPDF$HP[(i + 1) : n] # rewardDelays for i : (n-1) gap = (i + 1) : n tick
sum(values * weights)  / sum(weights)
})) / 0.1
meanTimeOutIti = sum(pmin(trialTicks$HP, t) * rewardDelayPDF$HP)
HPQuit = rep(HP[1] * (meanTimeOutIti + iti) / (meanTimeOutIti + iti*2), length(HP))
HP_[[h]] = HP
HPQquit_[[h]] = HPQuit
}
plotData = data.frame(Qwait = unlist(HP_), Qquit = unlist(HPQquit_), threshold = factor(rep(tList, times = unlist(lapply(HP_, length)))),
time = unlist(lapply(1 : nT, function(i )seq(0.1, tList[i], by = 0.1))))
plotData = gather(plotData, action, value, -c("time", "threshold"))
ggplot(plotData, aes(time, value,  color = action)) + geom_point() + facet_grid(~threshold)
libr
library("ggplot2")
library("dplyr")
library("tidyr")
# not so good to generate varibale in scripts, since sometimes it invloves variables you don't wont
######## condition varibles #########
conditions = c("HP", "LP")
conditionColors = c("#008837", "#7b3294")
######## timing variables ########
tMaxs = c(20, 40) # trial durations
blockMins = 7 # block duration in mins
blockSecs = blockMins * 60 # block duration in secs
iti = 2 # iti duration in secs
tGrid = seq(0, blockSecs, 0.1)
######### reward variable ########
tokenValue = 10 #value of the token
stepDuration = 0.5
########## supporting vairbales ########
# time ticks within a trial for timeEarnings or wtw analysis
trialTicks = list(
'HP' = round(seq(0, tMaxs[1], by = 0.1), 1),
'LP' = round(seq(0, tMaxs[2], by = 0.1), 1)
)
########## additional  variables for optimal analysis ########
# CDF of reward delays: p(t_reward <= T)
k = 4
mu = 0
sigma = 2
pareto = list()
pareto[['k']] = k
pareto[['mu']] = mu
pareto[['sigma']] = sigma
HP = 1 / trialTicks$HP[length(trialTicks$HP)]  * trialTicks$HP
LP = 1 - (1 + k * (trialTicks$LP - mu) / sigma) ^ (-1 / k)
LP[length(trialTicks$LP)] = 1
rewardDelayCDF = list(
HP = HP,
LP = LP
)
# library(ggplot2)
# source('plotThemes.R')
# plotData = data.frame(time = c(trialTicks$HP, trialTicks$LP),
#                       cdf = c(rewardDelayCDF$HP, rewardDelayCDF$LP),
#                       condition = c(rep('HP', length(trialTicks$HP)),
#                                     rep('LP', length(trialTicks$LP))))
# ggplot(plotData, aes(time, cdf, linetype = condition)) + geom_line() + xlab('Elapsed time / s') +
#   ylab('CDF') + ggtitle('Timing conditions') + saveTheme
#
# ggsave('../outputs/exp_figures/timing_conditions.png', width = 3, height = 2)
#  PDF of reward delays: p(t_reward = T)
#  make it discrete
HP = c(0, diff(rewardDelayCDF$HP))
LP = c(0, diff(rewardDelayCDF$LP))
rewardDelayPDF = list(
"HP" = HP,
"LP" = LP
)
# E(t_reward | t_reward <= T)
HP = cumsum(trialTicks$HP * rewardDelayPDF$HP) / cumsum(rewardDelayPDF$HP)
HP[1] = NaN
LP = cumsum(trialTicks$LP * rewardDelayPDF$LP) / cumsum(rewardDelayPDF$LP)
LP[1] = NaN
# no reward arrives before the first reward timing, so points before that turn to NAN
meanRewardDelay = list('HP' = HP, 'LP' = LP)
# rewardRate
HP = tokenValue * rewardDelayCDF$HP /
(meanRewardDelay$HP * rewardDelayCDF$HP + trialTicks$HP * (1 - rewardDelayCDF$HP) + iti)
LP = tokenValue * rewardDelayCDF$LP /
(meanRewardDelay$LP * rewardDelayCDF$LP + trialTicks$LP * (1 - rewardDelayCDF$LP) + iti)
# quitting before the first reward timing get 0 reward
HP[which(is.nan(HP))] = 0
LP[which(is.nan(LP))] = 0
rewardRate = list('HP' = HP, 'LP' = LP)
optimWaitTimes = list()
optimWaitTimes$HP = trialTicks$HP[which.max(HP)]
optimWaitTimes$LP = trialTicks$LP[which.max(LP)]
optimRewardRates = list()
optimRewardRates$HP = max(HP)
optimRewardRates$LP = max(LP)
# calculate wInis
wInisTheory = vector()
for(c in 1 : 2){
cond = conditions[c];
trialTick = trialTicks[[cond]]
thisDelayPDF = rewardDelayPDF[[cond]]
nTicks = length(trialTick)
# assume gamma = 0.9
gamma = 0.90
r = - log(gamma) / stepDuration
actionValueWaits = rep(0, nTicks)
for(k in 1 : nTicks){
actionValueWaits[k] = sum(tokenValue * exp(- (trialTick[k : nTicks] - trialTick[k]) * r)* thisDelayPDF[k : nTicks] / sum( thisDelayPDF[k : nTicks]))
}
junk = mean(actionValueWaits)
wInisTheory[c] = junk
}
wInis = vector(length = 2)
wInis[1] = 3 # value of waiting, since participants didn't know differences in conditions
wInis[2] = 2 # value of quitting, ensuring waiting first.
# calculate action value in the R learning, given the policy is always wait
# remember, here the average reward is for per 0.1s, namely per tick, not per s
n = length(trialTicks$HP)
HP = unlist(lapply(1 : (n - 1), function(i){
values = tokenValue - (trialTicks$HP[i : (n - 1)] - 0.1 * i + 0.1 + iti) *  rewardRate$HP[n]
weights = rewardDelayPDF$HP[(i+1) : n]
sum(values * weights)  / sum(weights)
})) / 0.1
n = length(trialTicks$LP)
LP = unlist(lapply(1 : (n - 1), function(i){
values = tokenValue - (trialTicks$LP[i : (n - 1)] - 0.1 * i + 0.1 + iti) *  rewardRate$LP[n]
weights = rewardDelayPDF$LP[(i+1) : n]
sum(values * weights)  / sum(weights)
})) / 0.1
tList = seq(5, 20, by = 5)
nT = length(tList)
HP_ = vector(mode = "list", length = nT)
HPQquit_ = vector(mode = "list", length = nT)
for(h in 1: nT){
t =  tList[h]
n = length(trialTicks$HP)
HP = unlist(lapply(1 : (t / 0.1), function(i){
values = rep(-rewardRate$HP[t / 0.1 + 1] *  (t -  0.1 * i + iti), n - i)# when get no rewards, wait for vein for (t - 0.1 * i) gap duration
values[1 : (t / 0.1 + 1 - i)] = tokenValue - (0.1 * (0 : (t / 0.1 - i)) + iti)* rewardRate$HP[t / 0.1 + 1]
weights = rewardDelayPDF$HP[(i + 1) : n] # rewardDelays for i : (n-1) gap = (i + 1) : n tick
sum(values * weights)  / sum(weights)
})) / 0.1
meanTimeOutIti = sum(pmin(trialTicks$HP, t) * rewardDelayPDF$HP)
HPQuit = rep(HP[1] * (meanTimeOutIti + iti) / (meanTimeOutIti + iti*2), length(HP))
HP_[[h]] = HP
HPQquit_[[h]] = HPQuit
}
plotData = data.frame(Qwait = unlist(HP_), Qquit = unlist(HPQquit_), threshold = factor(rep(tList, times = unlist(lapply(HP_, length)))),
time = unlist(lapply(1 : nT, function(i )seq(0.1, tList[i], by = 0.1))))
plotData = gather(plotData, action, value, -c("time", "threshold"))
ggplot(plotData, aes(time, value,  color = action)) + geom_point() + facet_grid(~threshold)
hist(rewardRate$HP)
tList = seq(5, 20, by = 5)
nT = length(tList)
HP_ = vector(mode = "list", length = nT)
HPQquit_ = vector(mode = "list", length = nT)
for(h in 1: nT){
t =  tList[h]
n = length(trialTicks$HP)
HP = unlist(lapply(1 : (t / 0.1), function(i){
values = rep(-rewardRate$HP[t / 0.1 + 1] *  (t -  0.1 * i + iti), n - i)# when get no rewards, wait for vein for (t - 0.1 * i) gap duration
values[1 : (t / 0.1 + 1 - i)] = tokenValue - (0.1 * (0 : (t / 0.1 - i)) + iti)* rewardRate$HP[t / 0.1 + 1]
weights = rewardDelayPDF$HP[(i + 1) : n] # rewardDelays for i : (n-1) gap = (i + 1) : n tick
sum(values * weights)  / sum(weights)
})) / 0.1
meanTimeOutIti = sum(pmin(trialTicks$HP, t) * rewardDelayPDF$HP)
HPQuit = rep(HP[1] * (meanTimeOutIti + iti) / (meanTimeOutIti + iti*2), length(HP))
HP_[[h]] = HP
HPQquit_[[h]] = HPQuit
}
plotData = data.frame(Qwait = unlist(HP_), Qquit = unlist(HPQquit_), threshold = factor(rep(tList, times = unlist(lapply(HP_, length)))),
time = unlist(lapply(1 : nT, function(i )seq(0.1, tList[i], by = 0.1))))
plotData = gather(plotData, action, value, -c("time", "threshold"))
ggplot(plotData, aes(time, value,  color = action)) + geom_point() + facet_grid(~threshold)
