---
output:
  html_document:
    css: style.css
    toc: true
    toc_depth: 2
---
<style>
    p {line-height: 2em;}
</style>

Loop for each episode, $i = 0, 1, 2, ...$, :   
\
<span style="color:white">space</span> $\pi_i(Wait, S_t) = \frac{1}{exp \{ tau \cdot \left[Q(Quit, S_t) - Q(Wait, S_t) - C_i\right] \}}$  
\
<span style="color:white">space</span> Loop for each step, $t = 0, 1, 2, ...$:  
<span style="color:white">spacespace</span> Unless $S_{t+1}$ is terminal <span style="color:grey">*#    either get a reward or quit*</span>  
<span style="color:white">spacespace</span> Choose and take $A_t$, observe $S_{t+1}$ and $R_{t+1}$  
\
<span style="color:white">space</span> $T_i$ = t+1 <span style="color:grey">*#    $T_i$ is the step index of the terminal state*</span>  
<span style="color:white">space</span>Loop for each step of episode, $t = 0, 1, 2, ..., T_iÔºç1$:  
<span style="color:white">spacespace</span>$G_t = \sum_{k= t+1}^{T_i} \gamma^{k-t-1} R_{k} + \gamma^{T_i- t}V_{i-1}(S_{iti})$     
<span style="color:white">spacespace</span>$Q_{i}(S_t, A_t) = Q_{i-1}(S_t, A_t) + \alpha(G_t - Q_{i-1}(S_t, A_t))$  
\
<span style="color:white">space</span>$V_{i}(S_{iti}) = V_{i-1}(S_{iti}) + \alpha(\gamma^{\tau_{iti}}G_t - V_{i}(S_{iti}))$<span style="color:grey">*#    update $V(S_{iti})$* </span>    
\
<span style="color:white">space</span>$Q_{i}(Quit) = Q_{i-1}(Quit) + \alpha(\gamma^{\tau_{iti}+1}G_t - Q_{i}(Quit))$<span style="color:grey">*#    counterfactual thinking* </span>

### notes:
[1] currently, we take each timestep as an undividable time unit and both waiting and quiting for one timestep cost one unit opportunity cost. It is equivalent to both quiting at and waiting until the end of the timestep.      
[2] accoring to the Sutton&Barto book, $R_{t+1}$ in Qlearning is not discounted in G_{t} in Qlearning yet is discounted in Rlearning.
[3] currently, $V(S_{iti})$ is asypototically equal to the average discounted rewards $J(\pi)$, and $Q(quit)$ is asypototically equal to $\gamma J(\pi)$.  
[4] The counterfactual learning component is important. Without it, the agent will only increase Qwait then he continously wait and get rewards, which leads to overpersistence. Also, the agent will decrease Qwait more than Qquit when he continously quit early and get nothing, which leads to underpersistence. 
